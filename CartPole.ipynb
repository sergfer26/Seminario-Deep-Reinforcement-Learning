{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole\n",
    "\n",
    "También conocido como el *problema péndulo invertido*, consiste en un poste, atado a un carro en superficie sin fricción con un centro de gravedad encima de su punto de pivoteo, que requiere ser estabilizado. El objetivo del agente es no dejar caer el poste, aplicando la fuerza apropiada al punto de pivoteo.\n",
    "\n",
    "El episodio termina cuando el poste se mueve 15º del eje verical o cuando el carro se mueve más de 2.4 unidades del centro.\n",
    "\n",
    "Para resolver el problema, haremos uso de la biblioteca de *Open AI* ``gym`` donde podemos encontrar el ambiente de Cart Pole junto con una visualización del problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente código se muestra la visualización de un episodio donde se toman acciones aleatorias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env.reset()\n",
    "\n",
    "for _ in range(500):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observaciones y acciones\n",
    "\n",
    "El Cart Pole tiene cuatro observaciones que se me muestran en la siguiente tabla:\n",
    "\n",
    "|Num| Observation         |Min |Max |\n",
    "|---|---------------------|----|----|\n",
    "| 0 | Cart position       |-4.8| 4.8|\n",
    "| 1 | Cart velocity       |-Inf| Inf|\n",
    "| 2 | Pole angle          |-24º| 24º|\n",
    "| 3 | Pole velocity at tip|-Inf| Inf|\n",
    "\n",
    "Al iniciar el ambiente, se obtine un vector de observaciones como se muestra a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04173305, -0.03951904, -0.01593346,  0.03068002])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son dos acciones posibles las que se pueden tomar en este pronblema como se muestra a continuación:\n",
    "\n",
    "|Num|Action                |\n",
    "|---|----------------------|\n",
    "| 0 |Push cart to the left |\n",
    "| 1 |Push cart to the right|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A apartir de cada accion que se ejecuta, es decir por cada ``step`` que realizamos recibimos información proporcionada por el ambiente  como se muestra a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.04252343, -0.23440891, -0.01531985,  0.31829349]), 1.0, False, {})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La información que nos proporciona es la siguiente:\n",
    "\n",
    "\n",
    "*   Un vector que representa a las observaciones.\n",
    "*   Una recompensa de valor  $1.0$.\n",
    "*   La señal  ``done flag = False``, que significa que el episodio no ha terminado.\n",
    "*   ``info`` ¿Un diccionario vacío?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.4710703e+00, -2.8120660e+38,  2.3025870e-01, -9.8757861e+37],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código anterior nos da una muetra como usar el método ``sample()`` de la clase ``Space``sobre ``action_space`` y ``observation_space``,  lo que hace es tomar de forma aleatoria acciones y observaciones, respectivamente. En particular tomar acciones de forma aleatoria es de gran utilidad cuando no se sabe como actuar para tener un mejor desempeño.\n",
    "\n",
    "## The random CartPole agent\n",
    "\n",
    "El siguiente código muetra como inicializar un agente que se comporte de forma aleatoria, en este caso no son relavantes las observaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio terminado en 20 pasos con recompensa de  20.00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward \n",
    "        total_steps += 1\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    print(\"Episodio terminado en %d pasos con recompensa de % .2f\" % (total_steps, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy on CartPole \n",
    "\n",
    "El método de *cross entropy* aplicado a Reinforcement Learning se clasifica como *model-free* y *policy-based*. Recordemos que los métodos en RL se pueden clasificar de la siguiente manera:\n",
    "\n",
    "* *Model-free* o *model-based*: La diferencia radica en que el primero toma las observaciones actuales para tomar una acción, por el\n",
    "\n",
    "* *Value-based* o *policy-based*\n",
    "* *On-policy* o *off-policy*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e5dce30e50e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mHIDDEN_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mPERCENTILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
